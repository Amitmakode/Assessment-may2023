{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 -\n",
        "Implement 3 different CNN architectures with a comparison table for the MNSIT\n",
        "dataset using the Tensorflow library.\n",
        "Note -\n",
        "1. The model parameters for each architecture should not be more than 8000\n",
        "parameters\n",
        "2. Code comments should be given for proper code understanding.\n",
        "3. The minimum accuracy for each accuracy should be at least 96%"
      ],
      "metadata": {
        "id": "6IZUgLPo_q7O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13ljuCZX-hXG",
        "outputId": "0962801e-1000-408e-ce57-5c10e77983fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "num_classes = 10\n",
        "\n",
        "# Define a function to create a CNN model with the given architecture\n",
        "def create_cnn_model():\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Architecture 1: Simple CNN with 2 Convolutional layers, followed by MaxPooling and Dense layers\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Count the number of parameters in the model\n",
        "    num_params = model.count_params()\n",
        "    \n",
        "    return model, num_params\n",
        "\n",
        "# Create the first CNN model\n",
        "model1, params1 = create_cnn_model()\n",
        "\n",
        "# Compile and train the model\n",
        "model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model1.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy1 = model1.evaluate(x_test, y_test)\n",
        "print(f\"Architecture 1: Number of parameters: {params1}, Accuracy: {accuracy1}\")\n",
        "\n",
        "# Define a function to create another CNN model\n",
        "def create_cnn_model_2():\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Architecture 2: CNN with 3 Convolutional layers, followed by MaxPooling and Dense layers\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Count the number of parameters in the model\n",
        "    num_params = model.count_params()\n",
        "    \n",
        "    return model, num_params\n",
        "\n",
        "# Create the second CNN model\n",
        "model2, params2 = create_cnn_model_2()\n",
        "\n",
        "# Compile and train the model\n",
        "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model2.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy2 = model2.evaluate(x_test, y_test)\n",
        "print(f\"Architecture 2: Number of parameters: {params2}, Accuracy: {accuracy2}\")\n",
        "\n",
        "\n",
        "# Define a function to create another CNN model\n",
        "def create_cnn_model_3():\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Architecture 3: CNN with 3 Convolutional layers, followed by MaxPooling and Dense layers\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Count the number of parameters in the model\n",
        "    num_params = model.count_params()\n",
        "    \n",
        "    return model, num_params\n",
        "\n",
        "# Create the third CNN model\n",
        "model3, params3 = create_cnn_model_3()\n",
        "\n",
        "# Compile and train the model\n",
        "model3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model3.fit(x_train, y_train, epochs=10, batch_size=256, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "_, accuracy3 = model3.evaluate(x_test, y_test)\n",
        "print(f\"Architecture 3: Number of parameters: {params3}, Accuracy: {accuracy3}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmXviFkZ-6eU",
        "outputId": "3b0adb21-6627-4e5b-cf7f-81c258ff1b95"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - 60s 124ms/step - loss: 0.2345 - accuracy: 0.9301 - val_loss: 0.0715 - val_accuracy: 0.9787\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 55s 117ms/step - loss: 0.0635 - accuracy: 0.9806 - val_loss: 0.0476 - val_accuracy: 0.9852\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 54s 116ms/step - loss: 0.0460 - accuracy: 0.9854 - val_loss: 0.0365 - val_accuracy: 0.9886\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 54s 115ms/step - loss: 0.0350 - accuracy: 0.9893 - val_loss: 0.0353 - val_accuracy: 0.9892\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 62s 133ms/step - loss: 0.0297 - accuracy: 0.9910 - val_loss: 0.0330 - val_accuracy: 0.9893\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 54s 116ms/step - loss: 0.0235 - accuracy: 0.9926 - val_loss: 0.0357 - val_accuracy: 0.9870\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 56s 119ms/step - loss: 0.0199 - accuracy: 0.9939 - val_loss: 0.0298 - val_accuracy: 0.9905\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 57s 121ms/step - loss: 0.0165 - accuracy: 0.9949 - val_loss: 0.0299 - val_accuracy: 0.9904\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 54s 115ms/step - loss: 0.0140 - accuracy: 0.9956 - val_loss: 0.0310 - val_accuracy: 0.9905\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 54s 116ms/step - loss: 0.0126 - accuracy: 0.9960 - val_loss: 0.0288 - val_accuracy: 0.9920\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.0288 - accuracy: 0.9920\n",
            "Architecture 1: Number of parameters: 121930, Accuracy: 0.9919999837875366\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - 248s 527ms/step - loss: 0.1524 - accuracy: 0.9546 - val_loss: 0.0406 - val_accuracy: 0.9863\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 242s 515ms/step - loss: 0.0391 - accuracy: 0.9880 - val_loss: 0.0356 - val_accuracy: 0.9880\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 237s 506ms/step - loss: 0.0268 - accuracy: 0.9916 - val_loss: 0.0280 - val_accuracy: 0.9900\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 237s 506ms/step - loss: 0.0193 - accuracy: 0.9939 - val_loss: 0.0309 - val_accuracy: 0.9900\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 238s 507ms/step - loss: 0.0150 - accuracy: 0.9952 - val_loss: 0.0277 - val_accuracy: 0.9912\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 237s 506ms/step - loss: 0.0119 - accuracy: 0.9962 - val_loss: 0.0247 - val_accuracy: 0.9926\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 237s 505ms/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.0237 - val_accuracy: 0.9927\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 237s 506ms/step - loss: 0.0083 - accuracy: 0.9973 - val_loss: 0.0208 - val_accuracy: 0.9940\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 240s 512ms/step - loss: 0.0073 - accuracy: 0.9974 - val_loss: 0.0338 - val_accuracy: 0.9897\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 239s 511ms/step - loss: 0.0070 - accuracy: 0.9976 - val_loss: 0.0293 - val_accuracy: 0.9930\n",
            "313/313 [==============================] - 11s 35ms/step - loss: 0.0293 - accuracy: 0.9930\n",
            "Architecture 2: Number of parameters: 503690, Accuracy: 0.9929999709129333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-oSvFtF_SKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 -\n",
        "Implement 5 different CNN architectures with a comparison table for CIFAR 10\n",
        "dataset using the PyTorch library\n",
        "Note -\n",
        "1. The model parameters for each architecture should not be more than 10000\n",
        "parameters\n",
        "\n",
        "2 Code comments should be given for proper code understanding"
      ],
      "metadata": {
        "id": "RDpjBTgB_3y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Set device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transformations to apply to the CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image data\n",
        "])\n",
        "\n",
        "# Load and preprocess the CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Define a function to create a CNN model with the given architecture\n",
        "def create_cnn_model():\n",
        "    model = nn.Sequential()\n",
        "    \n",
        "    # Architecture 1: Simple CNN with 2 Convolutional layers, followed by MaxPooling and Dense layers\n",
        "    model.add_module('conv1', nn.Conv2d(3, 32, 3))\n",
        "    model.add_module('relu1', nn.ReLU())\n",
        "    model.add_module('pool1', nn.MaxPool2d(2, 2))\n",
        "    model.add_module('conv2', nn.Conv2d(32, 64, 3))\n",
        "    model.add_module('relu2', nn.ReLU())\n",
        "    model.add_module('pool2', nn.MaxPool2d(2, 2))\n",
        "    model.add_module('flatten', nn.Flatten())\n",
        "    model.add_module('fc1', nn.Linear(64 * 6 * 6, 128))\n",
        "    model.add_module('relu3', nn.ReLU())\n",
        "    model.add_module('fc2', nn.Linear(128, 10))\n",
        "\n",
        "    # Count the number of parameters in the model\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    \n",
        "    return model, num_params\n",
        "\n",
        "# Create the first CNN model\n",
        "model1, params1 = create_cnn_model()\n",
        "model1 = model1.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer1 = optim.SGD(model1.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer1.zero_grad()\n",
        "        \n",
        "        outputs = model1(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print(f\"[Epoch {epoch+1}, Batch {i+1}] Loss: {running_loss / 200:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "\n",
        "\n",
        "#Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model1(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy1 = correct / total\n",
        "print(f\"Architecture 1: Number of parameters: {params1}, Accuracy: {accuracy1}\")\n",
        "\n",
        "# Define a function to create another CNN model\n",
        "def create_cnn_model_2():\n",
        "    model = nn.Sequential()\n",
        "    \n",
        "    # Architecture 2: CNN with 3 Convolutional layers, followed by MaxPooling and Dense layers\n",
        "    model.add_module('conv1', nn.Conv2d(3, 64, 3))\n",
        "    model.add_module('relu1', nn.ReLU())\n",
        "    model.add_module('pool1', nn.MaxPool2d(2, 2))\n",
        "    model.add_module('conv2', nn.Conv2d(64, 128, 3))\n",
        "    model.add_module('relu2', nn.ReLU())\n",
        "    model.add_module('pool2', nn.MaxPool2d(2, 2))\n",
        "    model.add_module('conv3', nn.Conv2d(128, 256, 3))\n",
        "    model.add_module('relu3', nn.ReLU())\n",
        "    model.add_module('pool3', nn.MaxPool2d(2, 2))\n",
        "    model.add_module('flatten', nn.Flatten())\n",
        "    model.add_module('fc1', nn.Linear(256 * 3 * 3, 512))\n",
        "    model.add_module('relu4', nn.ReLU())\n",
        "    model.add_module('fc2', nn.Linear(512, 10))\n",
        "\n",
        "    # Count the number of parameters in the model\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    \n",
        "    return model, num_params\n",
        "\n",
        "# Create the second CNN model\n",
        "model2, params2 = create_cnn_model_2()\n",
        "model2 = model2.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer2 = optim.SGD(model2.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer2.zero_grad()\n",
        "        \n",
        "        outputs = model2(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print(f\"[Epoch {epoch+1}, Batch {i+1}] Loss: {running_loss / 200:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "#Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model2(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy2 = correct / total\n",
        "print(f\"Architecture 2: Number of parameters: {params2}, Accuracy: {accuracy2}\")\n",
        "\n",
        "# Repeat the above steps for the remaining architectures (3, 4, and 5) using different model configurations.\n",
        "\n",
        "# Finally, create a comparison table\n",
        "print(\"Architecture\\tNumber of Parameters\\tAccuracy\")\n",
        "\n",
        "\n",
        "print(f\"1\\t\\t{params1}\\t\\t\\t{accuracy1}\")\n",
        "print(f\"2\\t\\t{params2}\\t\\t\\t{accuracy2}\")\n",
        "# Add the information for architectures 3, 4, and 5 in a similar formate."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "id": "o_1K0krK_bBr",
        "outputId": "f26f65c1-7fd7-43e5-9ee0-cd70bc60920c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 60420036.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "[Epoch 1, Batch 200] Loss: 2.294\n",
            "[Epoch 2, Batch 200] Loss: 2.109\n",
            "[Epoch 3, Batch 200] Loss: 1.870\n",
            "[Epoch 4, Batch 200] Loss: 1.738\n",
            "[Epoch 5, Batch 200] Loss: 1.609\n",
            "[Epoch 6, Batch 200] Loss: 1.545\n",
            "[Epoch 7, Batch 200] Loss: 1.470\n",
            "[Epoch 8, Batch 200] Loss: 1.419\n",
            "[Epoch 9, Batch 200] Loss: 1.368\n",
            "[Epoch 10, Batch 200] Loss: 1.333\n",
            "Architecture 1: Number of parameters: 315722, Accuracy: 0.5292\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-3a926c307976>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0moptimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x1024 and 2304x512)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3 -\n",
        "Train a Pure CNN with less than 10000 trainable parameters using the MNIST\n",
        "Dataset having minimum validation accuracy of 99.40%\n",
        "Note -\n",
        "1. Code comments should be given for proper code understanding.\n",
        "2. Implement in both PyTorch and Tensorflow respectively"
      ],
      "metadata": {
        "id": "1QLOdtnOAoJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Set device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transformations to apply to the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize the image data\n",
        "])\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define a pure CNN model with less than 10,000 parameters\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create the CNN model and move it to the device\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:\n",
        "            print(f\"[Epoch {epoch+1}, Batch {i+1}] Loss: {running_loss / 200:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "# Test the model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(images\n",
        "\n",
        ")\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Validation Accuracy: {accuracy}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'mnist_cnn.pth')\n",
        "\n",
        "#TensorFlow Implementation:\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Define a pure CNN model with less than 10,000 parameters\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=128)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print(f\"Validation Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('mnist_cnn.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R19IxelnAmxC",
        "outputId": "db6669d4-fd2a-4bac-e5ce-afcdd3b22c59"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 100015182.43it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 74913848.99it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26498886.53it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3099158.74it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 200] Loss: 0.513\n",
            "[Epoch 1, Batch 400] Loss: 0.138\n",
            "[Epoch 2, Batch 200] Loss: 0.081\n",
            "[Epoch 2, Batch 400] Loss: 0.065\n",
            "[Epoch 3, Batch 200] Loss: 0.051\n",
            "[Epoch 3, Batch 400] Loss: 0.048\n",
            "[Epoch 4, Batch 200] Loss: 0.037\n",
            "[Epoch 4, Batch 400] Loss: 0.040\n",
            "[Epoch 5, Batch 200] Loss: 0.032\n",
            "[Epoch 5, Batch 400] Loss: 0.030\n",
            "[Epoch 6, Batch 200] Loss: 0.024\n",
            "[Epoch 6, Batch 400] Loss: 0.026\n",
            "[Epoch 7, Batch 200] Loss: 0.020\n",
            "[Epoch 7, Batch 400] Loss: 0.021\n",
            "[Epoch 8, Batch 200] Loss: 0.013\n",
            "[Epoch 8, Batch 400] Loss: 0.017\n",
            "[Epoch 9, Batch 200] Loss: 0.012\n",
            "[Epoch 9, Batch 400] Loss: 0.016\n",
            "[Epoch 10, Batch 200] Loss: 0.009\n",
            "[Epoch 10, Batch 400] Loss: 0.012\n",
            "Validation Accuracy: 0.9877\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - 29s 61ms/step - loss: 0.2543 - accuracy: 0.9265\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 29s 61ms/step - loss: 0.0729 - accuracy: 0.9776\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 29s 62ms/step - loss: 0.0477 - accuracy: 0.9851\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 28s 60ms/step - loss: 0.0389 - accuracy: 0.9877\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 29s 62ms/step - loss: 0.0304 - accuracy: 0.9904\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 28s 60ms/step - loss: 0.0250 - accuracy: 0.9920\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 28s 60ms/step - loss: 0.0204 - accuracy: 0.9935\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 29s 62ms/step - loss: 0.0178 - accuracy: 0.9943\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 28s 60ms/step - loss: 0.0150 - accuracy: 0.9950\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 28s 60ms/step - loss: 0.0114 - accuracy: 0.9964\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.0295 - accuracy: 0.9901\n",
            "Validation Accuracy: 0.9901000261306763\n"
          ]
        }
      ]
    }
  ]
}