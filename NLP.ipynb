{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q-2. Take any pdf and your task is to extract the text from that pdf and store it in a\n",
        "csv file and then you need to find the most repeated word in that pdf."
      ],
      "metadata": {
        "id": "K0zWOAmweIni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6sEjwKifPht",
        "outputId": "312748c5-7b38-4237-e89b-af7a365ab3f5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "import pandas as pd\n",
        "\n",
        "# Open the PDF file\n",
        "pdf_file = \"/content/Data Science.pdf\"\n",
        "with open(pdf_file, \"rb\") as file:\n",
        "    pdf_reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "    # Extract text from each page of the PDF\n",
        "    text = \"\"\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        text += page.extract_text()\n",
        "\n",
        "# Store the extracted text in a CSV file\n",
        "csv_file = \"/content/sample_data/Data Science.csv\"\n",
        "data = {\"Text\": [text]}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(csv_file, index=False)\n",
        "\n",
        "# Find the most repeated word\n",
        "words = text.split()\n",
        "word_count = {}\n",
        "for word in words:\n",
        "    if word in word_count:\n",
        "        word_count[word] += 1\n",
        "    else:\n",
        "        word_count[word] = 1\n",
        "\n",
        "most_repeated_word = max(word_count, key=word_count.get)\n",
        "\n",
        "print(\"Most repeated word:\", most_repeated_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i5teDQ1fL6b",
        "outputId": "e29a0afc-8527-40dc-ea97-d5dd27040db7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most repeated word: the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-3. from question 2, As you got the CSV and now you need perform key word\n",
        "extraction from that csv file and do the Topic modeling\n"
      ],
      "metadata": {
        "id": "sVEwg_Lii06_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models.coherencemodel import CoherenceModel"
      ],
      "metadata": {
        "id": "AEPfJAbMgzNa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb5Cppi2iFfj",
        "outputId": "9799dc9f-b9cc-4a11-cc9f-7ba94e5e2566"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hdNDCsHiaXT",
        "outputId": "0e07d1f4-94be-4943-ab01-9f2409e7034f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJojS98Kihm7",
        "outputId": "927fbc80-2c90-4f6d-c5e9-561189907fa1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file\n",
        "csv_file = \"/content/sample_data/Data Science.csv\"\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Combine all text into a single string\n",
        "text = ' '.join(df['Text'].astype(str))\n",
        "\n",
        "# Tokenize the text into individual words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords and non-alphabetic characters\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
        "\n",
        "# Lemmatize the tokens\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Create a dictionary from the tokens\n",
        "dictionary = corpora.Dictionary([tokens])\n",
        "\n",
        "# Create a bag-of-words representation of the text\n",
        "corpus = [dictionary.doc2bow(tokens)]\n",
        "\n",
        "# Perform topic modeling using LDA\n",
        "num_topics = 5  # Set the number of topics\n",
        "lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary)\n",
        "\n",
        "# Get the most significant words for each topic\n",
        "topic_keywords = []\n",
        "for topic_id in range(num_topics):\n",
        "    words = lda_model.show_topic(topic_id, topn=5)\n",
        "    topic_keywords.append([word for word, _ in words])\n",
        "\n",
        "# Calculate coherence score for the model\n",
        "coherence_model = CoherenceModel(model=lda_model, texts=[tokens], dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "# Print the topic keywords and coherence score\n",
        "for topic_id, keywords in enumerate(topic_keywords):\n",
        "    print(f\"Topic {topic_id + 1}: {keywords}\")\n",
        "print(\"Coherence Score:\", coherence_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSw0ldVQhmwh",
        "outputId": "18fc442e-399a-40f1-cea1-13f31bcc416a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: ['e', 'n', 'b', 'r', 'q']\n",
            "Topic 2: ['e', 'n', 'r', 'q', 'k']\n",
            "Topic 3: ['q', 'r', 'e', 'n', 'company']\n",
            "Topic 4: ['e', 'r', 'n', 'q', 'b']\n",
            "Topic 5: ['e', 'r', 'n', 'q', 'b']\n",
            "Coherence Score: 0.2748554087892231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-4. Take any text file and now your task is to Text Summarization without using\n",
        "hugging transformer library\n"
      ],
      "metadata": {
        "id": "XjkkHywwi5Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "\n",
        "def read_text(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_sentences = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        words = [word.lower() for word in words if word.isalpha()]\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "        cleaned_sentences.append(words)\n",
        "    \n",
        "    return cleaned_sentences\n",
        "\n",
        "def calculate_sentence_similarity(sentence1, sentence2):\n",
        "    sentence1 = [word.lower() for word in sentence1]\n",
        "    sentence2 = [word.lower() for word in sentence2]\n",
        "    all_words = list(set(sentence1 + sentence2))\n",
        "    \n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "    \n",
        "    for word in sentence1:\n",
        "        vector1[all_words.index(word)] += 1\n",
        "    \n",
        "    for word in sentence2:\n",
        "        vector2[all_words.index(word)] += 1\n",
        "    \n",
        "    return 1 - cosine_distance(vector1, vector2)\n",
        "\n",
        "def create_similarity_matrix(sentences):\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "    \n",
        "    for i in range(len(sentences)):\n",
        "        for j in range(len(sentences)):\n",
        "            if i != j:\n",
        "                similarity_matrix[i][j] = calculate_sentence_similarity(sentences[i], sentences[j])\n",
        "    \n",
        "    return similarity_matrix\n",
        "\n",
        "def generate_summary(file_path, num_sentences):\n",
        "    text = read_text(file_path)\n",
        "    cleaned_sentences = preprocess_text(text)\n",
        "    similarity_matrix = create_similarity_matrix(cleaned_sentences)\n",
        "    \n",
        "    # PageRank algorithm to rank sentences based on similarity\n",
        "    scores = np.zeros(len(cleaned_sentences))\n",
        "    damping_factor = 0.85\n",
        "    epsilon = 0.001\n",
        "    \n",
        "    while True:\n",
        "        prev_scores = np.copy(scores)\n",
        "        \n",
        "        for i in range(len(cleaned_sentences)):\n",
        "            summation = 0\n",
        "            \n",
        "            for j in range(len(cleaned_sentences)):\n",
        "                if i != j:\n",
        "                    summation += (similarity_matrix[i][j] / np.sum(similarity_matrix[j])) * scores[j]\n",
        "            \n",
        "            scores[i] = (1 - damping_factor) + damping_factor * summation\n",
        "        \n",
        "        if np.sum(np.abs(prev_scores - scores)) < epsilon:\n",
        "            break\n",
        "    \n",
        "    ranked_sentences = [(score, sentence) for score, sentence in zip(scores, cleaned_sentences)]\n",
        "    ranked_sentences.sort(key=lambda x: x[0], reverse=True)\n",
        "    summary_sentences = [sentence for _, sentence in ranked_sentences[:num_sentences]]\n",
        "    summary = ' '.join([' '.join(sentence) for sentence in summary_sentences])\n",
        "    \n",
        "    return summary\n",
        "\n",
        "# Provide the file path and the desired number of summary sentences\n",
        "file_path = \"/content/fsds assessment.txt\"\n",
        "num_sentences = 3\n",
        "summary = generate_summary(file_path, num_sentences)\n",
        "\n",
        "print(\"Summary:\")\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "pUggF6GHkDiY",
        "outputId": "39e1bac6-7604-4b50-8372-dcb6529984d3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-467eb8c1d83b>\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/fsds assessment.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mnum_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Summary:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-467eb8c1d83b>\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(file_path, num_sentences)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                     \u001b[0msummation\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdamping_factor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdamping_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msummation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q-5. Now you need build your own language detection with the fast Text model\n",
        "by Facebook and\n"
      ],
      "metadata": {
        "id": "g6y_HeJEoMCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fmw2t48oOEh",
        "outputId": "e3d96631-7f49-4a54-81c7-917577fda393"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4393395 sha256=d3790507a865b8e491709d7044c674a20dff5b8df4f3585efa0ba49457664176\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "\n",
        "# Path to the training data file\n",
        "train_data = \"/content/fsds assessment.txt\"\n",
        "\n",
        "# Train the model\n",
        "model = fasttext.train_supervised(input=train_data, lr=1.0, epoch=25, wordNgrams=2, bucket=200000, dim=50, loss='softmax')\n"
      ],
      "metadata": {
        "id": "AZch6MiBoPoC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text to predict its language\n",
        "text = \"the, hindi\"\n",
        "\n",
        "# Predict the language\n",
        "prediction = model.predict(text)\n",
        "language = prediction[0][0].split(\"__\")[-1]\n",
        "\n",
        "print(\"Predicted Language:\", language)\n"
      ],
      "metadata": {
        "id": "HA-bLP3NoS4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MISEr_3kIAdG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}